[["index.html", "Kent R training Session 1 Prelude 1.1 A few words 1.2 A comment on the book structure", " Kent R training Lazaros Gonidis 2020-10-28 Session 1 Prelude 1.1 A few words regarding the structure of this course. Given that this course is tailored mainly for PhD students some minimal knowledge of statistics will be assumed. By minimal I do mean minimal. 1.2 A comment on the book structure The first six chapters in Day 1 of the training event. From the seventh chapter (session) onwards we are focusing on Day 2 of the training. We will first start by introducing R and RStudio together and I will frequently be saying just R or just RStudio but in reality I will be implying using RStudio to carry out R commands. Once you are introduced to R and RStudio we will then move to get familiarised to variables, vectors, dataframes, operations, and calculations. Many argue that you cannot teach the use of R without teaching programming. I disagree, and we will be doing very minimal, almost no programming at all. This is thanks to packages and functions that will be doing the heavy lifting for us. We will therefore learn how to install and use additional packages and benefit from their supply of functions. For example today we will be doing lots of work with data. This is because most of our work involves data preparation before we even carry out any analysis. We will therefore manipulating data using a famous package collection called tidyverse and most specifically dplyr and tidyr. It is through this work that you will become more confident and warm up to R. Once we are happy with how our data files look, we will proceed to data visualisations using another powerful package called ggplot2. This is also included in the tidyverse collection. Finally, we will open the floor to more practice examples and questions for you. You can even try todays techniques to some of your own data files and ask me questions in the process. This will conclude the first session and prepare us for the next session where we will be carrying out different types of analysis. "],["introduction.html", "Session 2 Introduction 2.1 Learning Objectives 2.2 Getting Things Ready 2.3 Starting with RStudio 2.4 Console 2.5 Environment 2.6 Files 2.7 Scripts 2.8 Exercise 2", " Session 2 Introduction 2.1 Learning Objectives In this first session we will introduce you to some basic features of R. This will allow you to write your first basic scripts. It should be noted that R is a versatile programming language with numerous features and capabilities. However, we will be focusing on concepts that are directly relevant to us, as Psychologists. Therefore, do not consider this course as an exhaustive guide to R. Instead treat it as a spotlight that will be shedding light on specific topics and techniques. By the end of this session you should have a clear understanding of the following concepts: Variables Operators Difference between script and console How to create a new script and save it How to add comments in your script How to see the content of a variable in the Environment How to write your own simple script to calculate an arithmetic expression 2.2 Getting Things Ready Ideally, we would like you to have your R related files organised. This will make your work easier to maintain but also help us to aid you when you get stuck. Therefore, we recommend creating a folder on your desktop called R training. Inside that folder you could be creating a new project for every session. This will make it much easier for you to navigate back and forth from topic to topic and look at different aspects of your work. Before you proceed with the following steps make sure you have created a folder named R training on your Desktop folder. 2.3 Starting with RStudio Run RStudio and create a new project by clicking on File &gt;&gt; New Project  .Then select New Directory and then New Project. Enter a Directory name. As mentioned above we recommend naming the folder Introduction to R and by clicking Browse make sure you place that folder in your R training folder that you created on your Desktop.Click Create Project to finalise creating your new project. You should now be seeing three main windows, the Console, the Environment, and the Files. 2.4 Console In the console we typically see the &gt; symbol and the cursor flashing next to it. Here we can type in R commands straight away and execute them by hitting Enter or Return. You can try it out. Go to your console and type the following and press Enter 3+2 Once you have done that you should be able to see the result of the calculation: [1] 5 So with the above command line we executed a simple calculation, an addition of 3 and 2. The symbol + is called an operator. R accepts a number of operators, for more information on operators see: https://www.datamentor.io/r-programming/operator/ . Most of them are straight forward and you have used them numerous times doing arithmetic calculations. For our purposes we will mostly be using the most common ones. Sometimes in programming we might want to carry out a calculation and use the results in many different occasions. We would not want to keep typing in the Console the same and the same operation, especially if it is a rather big expression. In order to save ourselves time and effort we can assign the result of an operation to variable. For example type the following in your Console and then hit Enter: p &lt;- 3+2 By doing the above, strangely enough we do not get an answer from R. Instead our Console looks like nothing really happened. In actual truth two things happened. First, R calculated the result of 3+2 and then it assigned the result of that calculation in to a variable called p. The reason that we do not see any result is because we did not instruct R to show the content of the variable p. If we want to see the content of the variable named p we have to type its name in the Console. Just type p and press enter. You should be able now to see the value that the variable p holds. [1] 5 This is extremely valuable as I can now use the symbol p to refer to that content anywhere I need to, for example I can calculate the surface of a square with size equal to 5 (or p in this case). Type the following and press Enter. surface &lt;- p*p surface [1] 25 So you see now that R calculated the result of 5x5, then assigned it to a new variable called surface and then returned the value of surface in the console, 25. We will be using variables a lot in our work in SP300 and it is worth noting that a variable is not restricted to numerical content. Other types of data can also be assigned to a variable. For example characters: day &lt;- &quot;Monday&quot; The above variable day is what we called a character variable. Notice how we used the \"\" symbols around the value that we want to assign. 2.4.1 Exercise 1 In the console carry out the following operation: 1. assign the value of 5 to a variable called f 2. assign the value of 8 to a variable called g 3. calculate the product of f*g, without assigning it to a new variable 2.5 Environment Usually the Environment window is on the top right side of our session. It includes valuable information about our work. For example you might already be able to see that it shows our two variable p and surface as well as their values. It also includes some other tabs next to the Environment but we will discuss those in the future. Lets have a look at what is already in our Environment. You should the value of f being 5 and the value of g being 8. Notice that the __f*g__ is not there. This is because we never assigned it to a variable. 2.6 Files At the bottom right of our session (usually) we can see our Files window. This includes information about the files that we are working on. At the moment you should be able to see just one file (probably), that should be your project name file. Next to Files we can also see a number of tabs that we will discuss in the future. 2.7 Scripts Once we get more familiarised with R we will need to type a number of commands so we can complete more complex tasks than just simple arithmetic calculations. Furthermore, we might want to be able to run again and again with different data files. So typing commands straight into the Console would be rather time wasteful. A more efficient way to work is to create a script instead that will allow us to carry out many commands at once as well as save our work for future use. Let us create our first script. Go to File &gt;&gt; New File &gt;&gt; R Script . That should create a new Untitled script in a new window that just opened above the Console window. We will be referring to this new window as the script window. Lets now go ahead and type a number of commands in our new script. Type the following commands as you see them below: ## hourly refers to the hourly pay of Â£9 per hour hourly &lt;- 9 ## hours refers to the number of hours I worked per day hours &lt;- 7 ## days refers to the number of days I worked in the current month days &lt;- 25 ## the total salary can be calculated by multiplying all the above salary &lt;- hours*hourly Now that you have typed all that nothing seems to be happening. This is because contrary to the Console, in a script a command is not executed when we press Enter or Return. Instead we have place our cursor in the line we want to execute and then click Run. When we click Run the lines is being executed and the cursor moves automatically to the next line, we click Run again and so on. Every time we click Run we can see the outcome in our Console, as well as in the Environment. Notice that our script also contains lines starting with #, these are comment lines that I include in my code to make it easier to read and understand. The use of # and comments is highly advisable and considered as **a good practice*. You should spend some time now to visually explore the screen in front you. Familiarise yourself with the different windows and click on different tabs to see what kind of information they contain. Also do not forget to save our script giving it a meaningful name. I suggest something like the most amazing script. You can save it by clicking File &gt;&gt; Save, and then type a name of your preference. Once you do that you should be able to see the new scripts in the Files window too. 2.8 Exercise 2 Create a new script and name it Exercise2. Write the necessary lines of code that can solve the following problem: The price for one cinema ticket is Â£10. The price for a large popcorn in Â£8. The price for one soft drink is Â£4. We want to calculate the total amount of money we spent if we bought 5 tickets, 3 popcorns and 4 soft drinks. Make sure you create a variable for each of the above costs, as well as one variable for the total. Also, make sure to include detailed comments as the ones in the example above. "],["session2.html", "Session 3 Using Vectors and Functions 3.1 Learning Objectives 3.2 Vectors 3.3 Creating vectors with random numbers 3.4 Frequency tables in R and Histograms 3.5 Exercise 3", " Session 3 Using Vectors and Functions 3.1 Learning Objectives In this session we be introducing the concepts of vectors and functions. These are two elements of R that will prove to be very useful in your day to day practice. We will also start working on some basic concepts of statistical exploration of data. One of the first steps we take as Psychologists when we want to analyse our data, is first to get a feel, or gist if you want, of how are data are distributed. By the end of this Lab session you should have a clear understanding of the following concepts: Vectors Functions and Arguments Creating Random Data for Practice Purposes Grouping Data using R Creating Frequency Tables using R Creating a basic Histograms using R Before we begin, do make sure you start a new project and name it Session 2. Note:If you are running RStudio Cloud do not start a new project, continue working on the same one from last week. 3.2 Vectors Let us assume that we asked 15 participants to report their age. One way to record this in R would be to use one variable per participant. It would look something like the following (DO NOT TYPE THE FOLLOWING IN RSTUDIO!!) age1 &lt;- 18 age2 &lt;- 20 age3 &lt;- 35 # and so on all the way down until we reach age15 &lt;- 32 As you can see this would take 15 variables. Not only it would look horrible but also it would be a very wasteful way to store our information. For cases where we want to store multiple observations single value variables are not the best way forward. Actually they are the worst way forward. Instead we can use a variable that can store many different values at once, a vector. The following is an example of a vector that stores our participants 15 ages. Please do type the following in your R script and do run each line to see the output. You should name your R script session2. # We are creating a vector c and we are populating it with 15 values # Then we assign it to a variable called Age Age &lt;- c(18, 18, 20, 32, 45, 33, 19, 19, 20, 58, 63, 21, 19, 19, 18) # If we call the variable Age we will get all 15 values Age [1] 18 18 20 32 45 33 19 19 20 58 63 21 19 19 18 # But we can also access any of the 15 entries we want by calling its &quot;location&quot; Age[7] [1] 19 Note we always use c() to define a vector. The same way we did calculations with simple value variables we can also do with vectors. For example, we may have a vector or prices for products and we may want to double the price of all of the products at once: # We are creating a vector c and we are populating it with 5 prices. # The prices could be in Â£ or any other currency (we only type the value in). # Then we assign our vector to a variable called Prices. Prices &lt;- c(115, 120, 200, 300, 500) # call Price to see the values in the console Prices [1] 115 120 200 300 500 # We now want a new variable that will include the old prices doubled. # We can call the new variable anything we want, let us call it Newprices Newprices &lt;- 2*Prices # call Price to see the values in the console Newprices [1] 230 240 400 600 1000 # But we can also access any of the 15 entries we want by calling its &quot;location&quot; Age[7] [1] 19 You see how powerful and handy vectors can be as they allow us to work with many values as once. We will be using vectors a lot in our work, as well as an extension of vectors called dataframes. (More about dataframes in the next sessions) 3.3 Creating vectors with random numbers As we do not have any real data yet, we might want to practice out learning with some made up data sets. That is absolutely fine but it poses a problem. If we want to create a vector with 200 made up values it will take us quite a while to actually type 200 values. Luckily, there is a solution to that. We can use an R function to create these numbers automatically. Let us see this in practice with a small example. # I am using a R function called sample.int() # This function creates random integer numbers numbers &lt;- sample.int(10, 5, replace = TRUE) numbers [1] 3 10 2 6 5 As mentioned above, I used a function called sample.int(). A function in R, is a mechanism that can perform a task. In this case our function can select an integer number randomly. A function also includes two brackets (). Within these brackets we can specify our own arguments in order to be clear what we want R to do for us when this function is used. Notice the number 10, this argument instructs our function that we want to pick randomly a number between 1 and 10. Notice the second number 5, this argument instructs our function to give 5 such numbers from 1 to 10. Notice the replace = TRUE part, this is another argument instructing R that numbers could be repeated. In other words, we can see the same number appearing more than once. 3.4 Frequency tables in R and Histograms Let us now proceed to a larger data set comprised of 200 random integers between 1 and 50. We will organise that data in 5 groups and produce a grouped frequency table. Then lastly, we will create a histogram with the same 5 groups. # Similarly as above, I will call sample.int() # But this time I need 200 numbers between 1 and 50. # See how my arguments have now changed. numbers &lt;- sample.int(50, 200, replace = TRUE) numbers [1] 14 25 26 27 5 27 28 9 29 35 8 26 7 42 9 19 36 14 17 43 39 12 15 32 42 [26] 45 7 9 41 10 23 27 7 27 32 38 25 34 29 5 8 12 13 18 33 27 25 38 21 15 [51] 41 47 26 31 16 30 6 43 8 22 22 39 31 48 17 50 49 34 4 13 5 25 22 25 32 [76] 46 25 23 35 40 48 30 12 31 46 30 35 14 29 32 7 3 23 15 21 37 8 10 50 42 [101] 44 34 10 22 12 20 46 17 46 35 40 46 30 15 24 49 23 43 7 29 15 23 26 38 46 [126] 32 7 27 42 5 6 16 24 32 21 11 36 44 46 19 25 39 26 9 7 34 48 13 19 47 [151] 39 4 1 40 30 30 25 16 24 11 48 20 40 3 29 36 44 22 49 42 20 11 8 46 21 [176] 45 2 43 13 46 6 8 44 32 36 45 14 16 23 33 40 40 10 25 8 18 9 7 7 10 # Now we will call a new function to help us group our data. # First put them in 5 groups. The numbers in c() declare the numbers that define each group. groups &lt;- cut(numbers, breaks = c(0,10,20,30,40,50), right = TRUE) # Now I can produce my frequency table using another function summary(groups) (0,10] (10,20] (20,30] (30,40] (40,50] 39 35 50 38 38 # We will now use a new function called hist that creates histograms. # Note that this function has to work with the original numbers # and not with the groups. hist(numbers, breaks = 5, labels = TRUE) Lets discuss the functions we used: sample.init() creates a sample of integer numbers cut() cuts our sample in smaller pieces. For arguments we had to provide our original sample (numbers). We then specified at what numbers we wanted the breaks to be (c(0,10,20,30,40,50)). Finally, we wanted the numbers on the right end of each group to be included in the count. summary() prints out the group frequency table, we only need one argument here, the variable that contains the grouped data (groups). hist() creates a histogram from our original sample (not the grouped one!!!). The arguments here were our sample (numbers), how many breaks we wanted (breaks = 5), and we wanted each bar to have its frequency at the top (labels = TRUE). As you can see we can define how many groups we want when we ask for our histogram. So when all we want is a histogram, then the cut() and summary() steps can be omitted. The histogram will do that behind the scenes and show us the frequencies in the graph. Try it out on your own by changing the number of breaks in the histogram. Notice each time your run your code you will get a different frequency distribution and a different histogram. This is because we create these 200 numbers randomly and each time we are getting a new set. 3.5 Exercise 3 Create a random set of 200 integers between 1 and 40. Group that data in 8 groups and produce the grouped frequency table. Then produce a histogram using these 8 groups. Spend some time to experiments with the number of bars in the histogram as well as generating new random numbers. This will allow you to become more confident with the use of arguments in functions. "],["session3.html", "Session 4 Packages 4.1 What we have learnt so far 4.2 Learning objectives 4.3 Lets get started 4.4 Measures of Central Tendency 4.5 Adding a new package 4.6 To sum up 4.7 Additional Exercises (spoiler alert: if you get stuck the answers are at the end of the pdf) 4.8 Solutions: SPOILER ALERT", " Session 4 Packages 4.1 What we have learnt so far In the first two labs (Week 2 and Week 3) we learnt about variables, vectors, and operators. Furthermore, we were introduced to the used of functions and their arguments. These were the fundamental steps that we had to take before we can start carrying out statistical tasks. In the last lab session, we began exploring some basic statistical tasks such as creating grouped frequency tables and histograms. These steps, even though simple, they allowed us to develop a sense of our data. Creating visual representations of our data is one of Rs strongest aspects. For the time being we only practiced with hist() and pie(), however as we become more confident with R we will also explore some additional features that R packages can give us. R in its core installation has a number of useful functions. However, these functions do not cover all the possible tasks we would wish to perform. In order to overcome this weakness R allows us to install and use additional packages. These packages are more specialised than the core R and can provide us with numerous new statistical tools. Today we will learn how to install and use such packages and see how they can provide us with useful new features. We will also see a number of examples of how to use R to acquire the mean, median, and mode of a sample. Finally, in order to make things more interesting we will use some experimental data that R offers by default. So today we will not be generating datasets comprised of random data. Instead we will be using some of the built-in datasets. 4.2 Learning objectives Installing R packages Calling R packages Use core R and packages to find measures of central tendency 4.3 Lets get started If you are working on RStudio Cloud skip to step 1 below For this session you will need to create a new project in RStudio. Run RStudio and create a new project by clicking on File &gt;&gt; New Project  .Then select New Directory and then New Project. Use packages as a Directory name. Click Browse and make sure you place that folder in your R training folder that you created on your Desktop.Click Create Project to finalise creating your new project. Now create a new script from File &gt;&gt; New File &gt;&gt; R Script. Save your new script under the name session4. One of the built-in datasets that R offers for free is called ToothGrowth. The ToothGrowth data set contains the result from an experiment studying the effect of vitamin C on tooth growth in 60 Guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, (orange juice (OJ) or ascorbic acid (a form of vitamin C and coded as VC). Even though this dataset is not the result of Psychological research it does have the structure of a data file from Psychological research where we administer an intervention in three different dosages and two different methods. Lets see how we can explore this dataset. In your new script type and run the following: data(&quot;ToothGrowth&quot;) This informs R that we want to work with the build-in dataset called ToothGrowth. If you have a look at your Environment you will now see under the section called Data an object called ToothGrowth. You will also see some additional information, it has 60 observations and 3 variables. If you also move your mouse pointer on top of ToothGrowth and leave it there you will get a tooltip window informing you that this is a data.frame. For now you can consider a data.frame as a table that contains information for more than one variables. If you click once on ToothGrowth then RStudio will open and display this data.frame for you. You can see it in a new tab that opened next to your script. You can see it has three columns, one per variable. The top row includes the variable names. len is the variable that contains information on the tooth growth, supp is the variable that contains information on the vitamin supplement used, and dose is the variable that describes the dosage that was administered. If you type and run each the following commands one at a time you will see each variable displayed in your console: ToothGrowth$len ToothGrowth$supp ToothGrowth$dose So if I want to work with one of the variables in a dataset I can call it by typing the name of the dataset, then type the $ symbol, and in after that type the name of the variable. Exercise 4 Try to create a histogram for the variable len of the dataset ToothGrowth. If you are not certain how to do this you can check the solution at the end of this pdf 4.4 Measures of Central Tendency Let us know see how we can use R functions to calculate the mean, median, and mode for a variable. If we want to find the mean of a variable we can use the function mean(). Similarly, if we want to find the median, we can use the median() function. Lets see them in practice for the variable len of the Toothgrowth dataset. # the following command will calculate the mean of len mean(ToothGrowth$len) # the following command will find the median value of len median(ToothGrowth$len) You can see now how with one line of code we can find in an instant a figure that would have taken us much more time and energy to calculate by hand. Now lets see if we can also find the mode in a similar way. Try the following command and see what happens. mode(ToothGrowth$len) You probably got the answer numeric. This is because the function mode() corresponds to a different task that what we hoped it would perform. Actually, the core installation of R does not have a function that can find the mode value of a variable. However, as we said earlier this is not a problem as we can install and run external packages that can add additional functionalities. 4.5 Adding a new package In order to have a function that can find the mode of our sample we need to install a new package called DescTools. We do that by clicking Tools in the menu and then Install Packages. Then where it says packages type in DescTools and click Install. If you are promtped with any questions then click on Yes. RStudio will download and install everything automatically and you should be ready to go once you can see the &gt; symbol in the console. Once you install a package you can use it in all future projects and scripts without the need to re-install it. Now we can tell R that we want to use this package in our script by adding and running the following command: library(DescTools) As a good practice we tend to declare the packages we want to use at the top of our scripts. We also need to include the command library(DescTools) only once per script, if we need it at all. Now we have the necessary function to find the mode of len by calling the function Mode() that comes with the package DescTools. Add the following command and run it to get the mode. library(DescTools) Mode(ToothGrowth$len) You should get the following output: ## Warning: package &#39;DescTools&#39; was built under R version 4.0.3 ## [1] 26.4 ## attr(,&quot;freq&quot;) ## [1] 4 This means that your mode is 26.4 and it has a frequency of 4. 4.6 To sum up As we progress through our course we will be using specific packages and functions. You do not need to memorise the packages names. We will be providing those for you. You will also not need to memorise the names of the functions for this term. We will be providing those too. 4.7 Additional Exercises (spoiler alert: if you get stuck the answers are at the end of the pdf) Another dataset that comes build-in with R is called iris. iris dataset gives the measurements in centimeters of the variables sepal length, sepal width, petal length and petal width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. The following exercises should be completed in one script and are in practice one exercise. They are broken down to smaller questions to make your work easier, as well as making it easier to refer to the solutions in the end. Exercise 5 Create a new script and name it session4_iris. Then type in and execute the right command in order to load the dataset iris Exercise 6 Explore the data frame called iris (you can find it in your Environment). You should be able to see that some of the included variables are Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width. Write the right code that would create one histogram for each of the above variables. Do spend some time to observe each histogram and think about how the scores are distributed and whether we have skewed distributions. (CAUTION: In R upper case and lower case letters do matter, make sure to type them exactly as you see them) Exercise 7 Type the right code in order to get the mean and the median for all the above variables. Exercise 8 Type the right code in order to get the mode for all the above variables. (Hint: do not forget the library and remember the importance of upper case letters.) . . . . . . . . . 4.8 Solutions: SPOILER ALERT Exercise 4 hist(ToothGrowth$len) Exercise 5 data(iris) Exercise 6 hist(iris$Sepal.Length) hist(iris$Sepal.Width) hist(iris$Petal.Length) hist(iris$Petal.Width) Exercise 7 mean(iris$Sepal.Length) ## [1] 5.843333 mean(iris$Sepal.Width) ## [1] 3.057333 mean(iris$Petal.Length) ## [1] 3.758 mean(iris$Petal.Width) ## [1] 1.199333 median(iris$Sepal.Length) ## [1] 5.8 median(iris$Sepal.Width) ## [1] 3 median(iris$Petal.Length) ## [1] 4.35 median(iris$Petal.Width) ## [1] 1.3 Exercise 8 library(DescTools) Mode(iris$Sepal.Length) ## [1] 5 ## attr(,&quot;freq&quot;) ## [1] 10 Mode(iris$Sepal.Width) ## [1] 3 ## attr(,&quot;freq&quot;) ## [1] 26 Mode(iris$Petal.Length) ## [1] 1.4 1.5 ## attr(,&quot;freq&quot;) ## [1] 13 Mode(iris$Petal.Width) ## [1] 0.2 ## attr(,&quot;freq&quot;) ## [1] 29 "],["session4.html", "Session 5 Tidyverse 1 5.1 Thinking ahead 5.2 Learning objectives 5.3 Our first data file 5.4 Correcting Variable Types 5.5 Recoding 5.6 Exercise 9", " Session 5 Tidyverse 1 5.1 Thinking ahead In this session we will start putting together everything we covered so far and start working on some of our own data. Here we should make a pause and think of the process we follow when we work with our data from start to finish. This process usually involves the following steps: load our data inspect the data file maybe create some basic graphs to see how things look or even calculate some basic descriptive statistics go through a process of cleaning (e.g., recoding, reverse scoring etc) restructuring the data file for the purposes of our analysis more graphs planned analysis report (if we havent run out of coffee) It is crucial to keep this process in mind as it will guide through the next two sessions. 5.2 Learning objectives Loading our data into R Inspect our Data frames Visually exploring our data Selecting and filtering our data Saving our data frames into files 5.3 Our first data file First, create a new project in our usual R training folder. Name the new folder and the new project session4. In our Files tab in our R training team you should be able to see a data file called alco_exp. This a data file from one of my experiments and I will provide some more detail as we go along. Download this file and place it in your session4 folder in the R Training folder. By doing so we can load into our scripts without having to specify the full path of the file. Now create a new script, also called session4. Type the following commands and run them one line at a time: library(tidyverse) ### load our &quot;alcohol_exp,csv&quot; into a df called &quot;alc_df&quot; alc_df &lt;- read_csv(&quot;alcohol_exp.csv&quot;) You should be able to see the following output in your console, or at least parts of it: ## Warning: package &#39;tidyverse&#39; was built under R version 4.0.3 ## Warning: replacing previous import &#39;vctrs::data_frame&#39; by &#39;tibble::data_frame&#39; ## when loading &#39;dplyr&#39; ## -- Attaching packages ----------- ## v ggplot2 3.3.2 v purrr 0.3.4 ## v tibble 3.0.4 v dplyr 1.0.0 ## v tidyr 1.1.0 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.5.0 ## Warning: package &#39;tibble&#39; was built under R version 4.0.3 ## -- Conflicts -------------------- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## Parsed with column specification: ## cols( ## participant = col_double(), ## group = col_double(), ## session = col_double(), ## image = col_character(), ## duration = col_double(), ## response = col_character(), ## thisRepN = col_double(), ## thisTrialN = col_double(), ## thisN = col_double(), ## thisIndex = col_double(), ## response.rt = col_double(), ## meditation = col_character(), ## gender = col_double(), ## age = col_double() ## ) You can also click on the alc_df object in your Environment, this will open the data frame in a tab where you can inspect it. If you hover your mouse pointer at the top of each column you should see information about each variable. Another way to get that information is by running the function str() and using the name of your data frame as an argument. Try this out and see what you get. Also, experiment with the following functions and try to guess what they produce: dim() summary() names() nrow() ncol() 5.4 Correcting Variable Types From exploring this data frame I can see that we need to adjust the type of some variables. This can be crucial for future sessions where the type of a variable can drastically affect the analysis output. For example, the variable meditation is showing as character, meaning that its content is characters. In reality this is actually a factor with two levels and R should be notified for this. Going through your variables and checking that that are assigned the right type is an important step that will also make you more skilful in the use of R. Add the following commands in your existing scripts: ### changing meditation to a factor variable alc_df$meditation = as.factor(alc_df$meditation) ### check the number of levels and their labels levels(alc_df$meditation) nlevels(alc_df$meditation) Which other variables in the data frame you would say need to be changed to factors? Go ahead and change them. 5.5 Recoding Another process that takes place while preparing data file is recoding. As a good practice we tend to recode a variable into a new variable and preserving the original variable. Tidyverse has a function for this called recode(). However, what we also have to start thinking now is that we want to recode an entire column (vector) of data. Traditionally in R we would need a programming technique called loop. But I promised you that we would keep programming to a minimum. Luckily, tidyverse (actually dplyr) comes to the rescue again with a function called mutate. Lets see an example below: ### creating a new data frame named Gender that contains the recoded gender variable alc_df2 &lt;- mutate(alc_df, Gender = recode(gender, &#39;1&#39; = &quot;Male&quot;, &#39;2&#39; = &quot;Female&quot;)) Upon inspecting the new dataframe called alc_df2 I see that the variable Gender is not a factor but a character. I can change that to using the as.factor function: ### creating a new data frame named Gender that contains the recoded gender variable alc_df2$Gender &lt;- as.factor(alc_df2$Gender) Next, I would like to keep only specific variables. I want to keep the variables ### creating a new data frame named Gender that contains the recoded gender variable alc_df3 &lt;- select(alc_df2, participant, group, session, image, duration, response, response.rt, meditation, age, Gender) This was not really a very efficient way to do that. Select() has a number of option to make our life easier. Especially if we notice that I only skipped four variables. So another way to do that would have been: ### creating a new data frame named Gender that contains the recoded gender variable alc_df4 &lt;- select(alc_df2, c(participant:response, response.rt:Gender)) Or even another way: ### creating a new data frame named Gender that contains the recoded gender variable alc_df5 &lt;- select(alc_df2, !7:10) For some strange reason I also want to filter out any participant with participant number higher than 200. Lets say there a software problem that day and I want all these participants out of my analysis: ### creating a new data frame named Gender that contains the recoded gender variable alc_df6 &lt;- filter(alc_df5, participant &lt; 200) Now that we have a data frame with the data as I want it I can save it to file. All the work we have done so far is with data frames. Data frames are stored in memory so remember to store them to actual files before you exit RStudio (although I am kind of lying to you, but lets pretend you believe me for now). ### creating a new data frame named Gender that contains the recoded gender variable write_csv(alc_df6, &quot;finalfile.csv&quot;) NOTE I have actually been horrible to you and we could have done all the above work in a much better way using what is called pipe operator. But for the time being, we need the practice so we will introduce the piper operator in the next session. 5.6 Exercise 9 Load the file alcohol_exp to a data frame called df. Then write the appropriate commands to complete the following steps: Set group, session, meditation, and gender as factors Keep only these four variables and response.rt Recode session from 1 and 2 to pre and post Recode group from 1 and 2 to high and low Keep only Female participants Keep only response.rt below 1.5 save your final data frame into a file called wastedresources.csv "],["session5.html", "Session 6 Tidyverse 2 6.1 Reflecting on the last session 6.2 Pipe Operator %&lt;% 6.3 Exercise 10 6.4 Grouping, spliting, and summarizing variables across out data frame", " Session 6 Tidyverse 2 6.1 Reflecting on the last session We have managed to do quite a few tasks already. However, we also managed to create a proper in the process. Our environment must be full of objects that I will not be using. And even though I could remove them it is best if I actually do not create them in the first place. That can be achieved with the use of the pipe operator or else %&lt;% (ctrl+shift+m). ##Learning objectives 1. Work more efficiently using tidyverse (dplyr and tidyr) 2. manipulating data 3. summarising data (brief, we will start from this in Day 2) Note: In this session we will be focusing on describing a number of tasks and then discussing how we can best perform them. We have already introduced enough concepts to do really meaningful work. Now we have to put our progress to the test. 6.2 Pipe Operator %&lt;% Lets start with seeing how the pipe operator works. If you think about the last exercise in the previous session you can remember that it took a number of steps where we kept assigning our work to a new data frame. With the use of %&lt;% we can write all these steps one under the other in one chunk of code. Exercise 9 Load the file alcohol_exp to a data frame called df. Then write the appropriate commands to complete the following steps: Set group, session, meditation, and gender as factors Keep only these four variables and response.rt Recode session from 1 and 2 to pre and post Recode group from 1 and 2 to high and low Keep only Female participants Keep only response.rt below 1.5 save your final data frame into a file called wastedresources.csv library(tidyverse) df &lt;- read_csv(&quot;alcohol_exp.csv&quot;) df3 &lt;- df %&gt;% mutate(group = as.factor(group)) %&gt;% mutate(session = as.factor(session)) %&gt;% mutate(meditation = as.factor(meditation)) %&gt;% mutate(gender = as.factor(gender)) %&gt;% select(group, session, meditation, gender, response.rt) %&gt;% filter(gender == 2) %&gt;% filter(response.rt &lt; 1.5) %&gt;% ### this could be done with the above mutate(gender = recode(gender,&#39;1&#39; = &quot;Male&quot;, &#39;2&#39;=&quot;Female&quot;)) %&gt;% mutate(gender = recode(group,&#39;1&#39; = &quot;High&quot;, &#39;2&#39;=&quot;Low&quot;)) ### and just for fun lets also create a histogram of reaction times. hist(df3$response.rt) ### lets save the data file write_csv(df3, &quot;wastedresources.csv&quot;) Now your turn 6.3 Exercise 10 Load the file alcohol_qualtrics.csv to a data frame Drop all the variables after the column 17 Make sure gender and smoke are factors Remove any participants with participant number below 100 and above 300 Add a column that is the sum of Days1 to Days4 Reverse code the variable Freq4 Add a column that is the mean of Freq1, Freq2, Freq3, and the above reversed coded variable Save the final file using a file name of your choice. 6.4 Grouping, spliting, and summarizing variables across out data frame Other jobs that we usually carry out when preparing out data files include aggregating variables, grouping by a variable, or splitting our file. This can also be done easily in R using the function group_by(). Lets see an example here: library(tidyverse) df &lt;- read_csv(&quot;alcohol_exp.csv&quot;) df %&gt;% group_by(gender) %&gt;% summarize(mean = mean(response.rt), sd = sd(response.rt), N = n()) %&gt;% ungroup() I can also add a new column in my data frame that will include information on the mean per gender. df %&gt;% group_by(gender) %&gt;% mutate(mean = mean(response.rt)) %&gt;% ungroup() Not that meaningful to be honest. Lets try again but this time grouping by participant. df %&gt;% group_by(participant) %&gt;% mutate(mean = mean(response.rt)) %&gt;% ungroup() This is much more useful from our perspective. Which brings us to the point that we can pretty much reshape a data frame in any way we want. So your work with R should be guided by your end goal, which is how you want your data file to look before your analysis. "],["session7.html", "Session 7 Bringing Everything Together. Part I 7.1 Reflecting on Day 1 7.2 Learning Objectives 7.3 Working with the file 7.4 Exercise 11", " Session 7 Bringing Everything Together. Part I 7.1 Reflecting on Day 1 In Day 1, we generally introduced individual packages and functions. We also introduced Tidyverse, or to be fair, some aspects of it. In Day 2, the approach will be different. Our work will be aim driven and in each of the following sessions we will have a driving aim. 7.2 Learning Objectives Working with the datafile alcohol_exp.csv we want to bring the dataset to a form where we could carry out analysis of variance (analyses if we want to be precise). This dataset includes a number of variables. Below we will discuss those of interest. We will exclude some of the variables as they were generated by Psychopy and are actual psychological variables. A few words on the dataset variables Participant: a unique id that was assigned to each participant. This should be numeric. group: participants were either assigned to manipulation group 1 or 2. Group 1 did mindfulness meditation, group 2 did not (This is in practice the same variable as meditation). This variable should be a factor. session: each participant completed the experimental task twice. Once before the manipulation and once after. This should also be a factor. image: The image file that was shown during a trial of the experiment. In total 16 different images, eight alcohol related and eight neutral. We can tell if an image is alcohol related if they include the character 1 before the .jpg part. If the character is 2 then the image was neutral. duration: We have a total of seven durations (in seconds). These are 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6. These can be treated as factor or as numerical. We will discuss the difference during the session. response: during each trial participants had to respond by pressing either s or l depending on whether they felt the stimulus stayed on the screen for short or long duration. This is a character variable but we should convert it to numerical so we can extract mean responses. meditation: discussed above, the same as group. This should be a factor. gender: self-explanatory age: self-explanatory A few words on the design For the above, we can say that participant, group, meditation, are between-subjects variables. Whereas, session, image, duration, are within-subjects variables. Our ultimate aim here is to carry out a three-way ANOVA with session, image, and duration as our IVs and response as our DV. Traditionally, you would need to have your file in the wide format to carry our this analysis in SPSS, where you would employ repeated measures and then declare your between-subjects variables. R operates in a different way, we have to have our dataset in the long format where our measurement in only in one column. We have to keep that in mind while we are working through the dataset. Our dataset should look like this just before our analysis: Steps to take We are keeping variables participant, meditation, and session. Although we changed their location in the dataset (we will use the function relocate()). We have a new variable called salience, this was created from the information included in the variable image (we will use plenty of exciting toys here such as mutate(), str_sub(), and case_when()). We have a new variable called durationms, this is duration in ms (pretty much multiplied duration by 1000). We have a new variable called ratio. Here we will have to carry out the bulk of the work. We will need to group and aggregate our data after we have numerical value instead of a character one for our response. Functions to be used mutate() str_sub() case_when() as.factor() (and other as.type functions) relocate() group_by() summarise() ungroup() (and possibly some others too) Packages to be used 1. tidyverse 2. rstatix 7.3 Working with the file At this point it is best to start a new project and call it Day 2. Once you have that up and running then create our first script for the day, I suggest naming it 3wANOVA. Let us start by loading the data file and checking that our variables are of the correct type. library(tidyverse) library(rstatix) ## load the data file to the dataframe called exp exp &lt;- read_csv(&quot;alcohol_exp.csv&quot;) ##inspect exp head(exp) ## contrary to Day 1 approach we will not be constantly creating new dataframes ## we will create only one new dataframe called exp2 ## lets begin by assigning correct variable types exp2 &lt;- exp %&gt;% mutate(group = as.factor(group)) %&gt;% mutate(session = as.factor(session)) %&gt;% mutate(meditation = as.factor(meditation)) %&gt;% mutate(gender = as.factor(gender)) %&gt;% mutate(group = as.factor(group)) Note the above executed pipe block could be written as one mutate library(tidyverse) library(rstatix) ## load the data file to the dataframe called exp exp &lt;- read_csv(&quot;alcohol_exp.csv&quot;) ##inspect exp head(exp) ## contrary to Day 1 approach we will not be constantly creating new dataframes ## we will create only one new dataframe called exp2 ## lets begin by assigning correct variable types exp2 &lt;- exp %&gt;% mutate(group = as.factor(group), session = as.factor(session), meditation = as.factor(meditation), gender = as.factor(gender), group = as.factor(group)) And to be honest it could written in even simpler way: library(tidyverse) library(rstatix) ## load the data file to the dataframe called exp exp &lt;- read_csv(&quot;alcohol_exp.csv&quot;) ##inspect exp head(exp) ## contrary to Day 1 approach we will not be constantly creating new dataframes ## we will create only one new dataframe called exp2 ## lets begin by assigning correct variable types exp2 &lt;- exp %&gt;% mutate(across(c(group , session, meditation, gender, group), as.factor)) As you work more with tidyverse you can decide on the style and approach that suites you most. For the purposes of this training we will stick to the first approach so you can work out the steps that you followed. Our variables should fine for now. Looking at the image provided, we want participant, meditation, and session to be the first three variables. Group seems to have been dropped. We will move meditation after participant and group at the end of the dataframe ## we will continue working on the dataframe exp2 ## relocating meditation and group exp2 &lt;- exp2 %&gt;% relocate(meditation, .after = participant) %&gt;% relocate(group, .after = last_col()) We now want to create a new variable called salience. We know that the third character of the image filename represents alcohol or neutral image. We will exract that information and then create a new variable called salience. ## we will use str_sub() to extract the third character ## based on the value of that character we will assign a value ## in our salience variable. Note how we are using case_when() exp2 &lt;- exp2 %&gt;% mutate(type = str_sub(image,3,3)) %&gt;% mutate(salience = case_when( type == &#39;2&#39;~&quot;Neutral&quot;, type ==&#39;1&#39;~&quot;Alcohol&quot; )) We will now proceed to recode the response from s and l to 0 and 1. We will also create a new variable for duration in ms and not in seconds. ## a simple mutate here would also do the job ## each mutate is creating a new variable exp2 &lt;- exp2 %&gt;% mutate(durationms = duration*1000) %&gt;% mutate(responsenum = case_when( response == &#39;l&#39;~ 1, response == &#39;s&#39;~ 0 )) We now have to deal with the fact that our DV is just trial responses. We want to aggregate these responses across participant, meditation, session, salience, and durationms. Here we will also create a new dataframe as our structure will change. ## we will group our data using the variables above ## and then we will calculate the mean of the responses ## these mean values will be assigned to the the new variable called ration ## do not forget to ungroup at the end of your grouping work ## we carried the variable session forward for your upcoming exercise ## we will drop it from the grouping as we do not need it for this ## three-way anova exp3 &lt;- exp2 %&gt;% group_by(participant, meditation, salience, durationms) %&gt;% summarise(ratio = mean(responsenum)) %&gt;% ungroup() ## make sure to inspect the exp3 dataframe to check that everything is in order ## you should be able to see that grouping and aggregating ## also dropped all other undeclared variables Lets do a final check on variable types and correct any discrepancies. This is also a good point to take a short break before we continue with the exrcise. 7.4 Exercise 11 Load the datafile stroopy stroop.csv and assign it to a dataframe called stroop. This contains the data from a Stroop experiment and you should go through the process of preparing it for a two-way ANOVA. You should complete the following steps, which are not given necessarily in the correct order. Create a group variable by recoding the participant variable. Participants with value below 2000 should be in the control group. Participants with value above 2000 should be in the gambling group. Create a variable that will denote the type of stimulus (call it salience). This information is included in the Image variable. If an image starts with G then it is a gambling image, if it starts with N then it is a neutral image. Keep only the correct responses, this information is in the stroop_resp.corr variable. Correct responses are recorded with 1. Discard the first two trials per participant. This information is in the stroop_trials.thisTrialN variable. Keep only the responses with a reaction time between 300 and 3000ms. This information is in the stroop_resp.rt (which is in seconds though). A variable with response times in ms would make more sense. Call it RT. Aggregate your RT by participant, group, and salience. "],["session8.html", "Session 8 Bringing Everything Together. Part II 8.1 Learning Objectives 8.2 Continuing with the exercise 10 and two-way ANOVA 8.3 Exercise 12", " Session 8 Bringing Everything Together. Part II 8.1 Learning Objectives We will learn how to carry out a two-way mixed anova and produce useful descriptives and plots. 8.2 Continuing with the exercise 10 and two-way ANOVA Our dataframe should be all ready now. You might want to save it to a datafile. We will be going through a number of steps to check ANOVA assumptions and follow-up with any necessary post-hoc analysis. As we do that we will also be generating some graphs which will give us the opportunity to discuss ggplot2 and an extension of it called ggpubr. Our main package for the anova will be the rstatix package. First, lets check that you completed exercise 10. library(tidyverse) stroop &lt;- read_csv(&quot;stroopy stroop.csv&quot;) stroop &lt;- stroop %&gt;% mutate(RT = stroop_resp.rt*1000, group = case_when( participant &lt; 2000 ~ &quot;control&quot;, participant &gt;= 2000 ~ &quot;gambling&quot;), salience = case_when( str_sub(Image,1,1) == &#39;G&#39;~&quot;Gambling&quot;, str_sub(Image,1,1) == &#39;N&#39;~&quot;Neutral&quot;)) %&gt;% filter(stroop_resp.rt &gt; 0.3 &amp; stroop_resp.rt &lt; 3.0 &amp; stroop_trials.thisTrialN &gt; 2) stroop2 &lt;- stroop %&gt;% group_by(participant, group, salience) %&gt;% summarise(RT = mean(RT)) %&gt;% ungroup() We will now start working on our two-way analysis of variance. First we will load the necessary libraries and convert our IVs into factors. library(tidyverse) library(ggpubr) library(rstatix) ## make sure you have run the code above ## converting group and salience to factors stroop2 &lt;- stroop2 %&gt;% mutate(group = as.factor(group), salience = as.factor(salience)) ## let&#39;s inspect our RT visually ggboxplot(stroop2, x = &quot;salience&quot;, y = &quot;RT&quot;, color = &quot;group&quot;) ##perhaps it would be more meaningful to also see boxplots per group ## we will also assign it to a variable so we can use it later box1 &lt;- ggboxplot(stroop2, x = &quot;group&quot;, y = &quot;RT&quot;, color = &quot;salience&quot;) We will now proceed with checks for outliers and anova assumptions. As this is a 2x2 some of the following checks are not meaningful but we will include them for reference. ## check for outliers stroop2 %&gt;% group_by(salience, group) %&gt;% identify_outliers(RT) ## Normality check stroop2 %&gt;% group_by(salience, group) %&gt;% shapiro_test(RT) ## you may also want to check this visually ## using a QQplot ggqqplot(stroop2, &quot;RT&quot;, ggtheme = theme_bw()) + facet_grid(group ~ salience) ## Homogeneity of variance assumption stroop2 %&gt;% group_by(salience) %&gt;% levene_test(RT ~ group) ## the assumption of sphericity will be checked during anova Assuming everything is in order we can proceed to carrying out the anova. ## anova time # first describe the model anova.model &lt;- anova_test( data = stroop2, dv = RT, wid = participant, ## this is our participant id variable between = group, within = salience ) # then get the anova table get_anova_table(anova.model) As we can see we have a significant interaction as well as a main effect. We have to follow up the interaction. ## First, We will need to regroup our data simple.model &lt;- stroop2 %&gt;% group_by(group) %&gt;% anova_test(dv = RT, wid = participant, within = salience) %&gt;% get_anova_table() %&gt;% adjust_pvalue(method = &quot;bonferroni&quot;) simple.model ## or we could run pairwise comparisons pair.model &lt;- stroop2 %&gt;% group_by(salience) %&gt;% pairwise_t_test(RT ~ group, p.adjust.method = &quot;bonferroni&quot;) pair.model ## it would also be useful to report means and sd stroop2 %&gt;% group_by(salience, group) %&gt;% get_summary_stats(RT, type = &quot;mean_sd&quot;) From here we can also generate plots if we want to as we already all the necessary stats figures. pair.model &lt;- pair.model %&gt;% add_xy_position(x = &quot;salience&quot;) box1 + stat_pvalue_manual(pair.model, tip.length = 0, hide.ns = TRUE) + labs( subtitle = get_test_label(anova.model, detailed = TRUE), caption = get_pwc_label(pair.model) ) ## or with a barchart box2 &lt;- ggbarplot(stroop2, x = &quot;group&quot;, y = &quot;RT&quot;, width = 0.5, color = &quot;salience&quot;, fill = &quot;salience&quot;, position = position_dodge(0.5), add = &quot;mean_se&quot;, add.params = list(color = &quot;black&quot;), label = TRUE, lab.vjust = -2, lab.nb.digits = 2) box2 ## Let us also save our final stroop2 dataframe write_csv(stroop2, &quot;stroopfinal.csv&quot;) As mentioned earlier ggpubr is a package that calls ggplot2. It is really versatile and can cover all our needs in terms of analysis and publications. However, ggplot2 offers many more features that you can try to explore on your own. 8.3 Exercise 12 Now that you are familiar with the two-way anova you should attempt the three-way anova based on the data that we prepared in the previous session. "],["session9.html", "Session 9 Restructuring and Merging Data. 9.1 Learning Objectives 9.2 Restructuring Data", " Session 9 Restructuring and Merging Data. 9.1 Learning Objectives In this session we will learn how to change from wide to long data and vice versa. We will also learn how to merge two different datasets and carry out a correlation analysis. 9.2 Restructuring Data As we have seen so far we have been working with datasets in long format. Meaning we have different columns to denote our variables and our dependent variable only in one column. However, there will be cases where we will need to use what is called a wide format where we will need to have measures across different columns. For example, in the previous stroop paradigm we may want to have one column for the salient reaction times and one column for the **neutral* reaction times. One reason why we may need this is because we may want to calculate a new variable that will be the difference between the two variables. That way we will end up having one row per participant which will make our dataset ideal for merging with another dataset that contains questionnaires responses. Our goal for this session will be to merge experimental and survey data in one file and carry out correlational analysis. For restructuring our data we will be using pivot_wider() and pivot_longer(). Both are part of tidyverse and specifically tidyr. They are new implementations and will probably replace spread() and gather(). Most tutorials on the web use spread and gather. We will use both the new and old approach. Let us continue our work from the previous session and work with the file stroopfinal. library(tidyverse) dflong &lt;- read_csv(&quot;stroopfinal.csv&quot;) We will now convert our dataframe from long to wide format. We will do using two different ways. Spread() and gather() are being depreciated but we still use them as an example. ## we now want to bring the file to a form that will have two RTs columns ## one for salience trials and one for neutral trials dfwide &lt;- dflong %&gt;% pivot_wider(names_from = salience, values_from = RT) ## we could do the same with spread() dfwide2 &lt;- dflong %&gt;% spread(salience, RT) Just for practice we can revert back from wide to long format. ## Let us now try to convert the wide file back to long dflong2 &lt;- dfwide %&gt;% pivot_longer(cols = c(Gambling, Neutral), names_to = &quot;Salience&quot;, values_to = &quot;Reaction&quot;) ### It would be very similar using gather dflong3 &lt;- dfwide %&gt;% gather(key = &quot;Salience&quot;, value = &quot;Reaction&quot;, Gambling, Neutral) ## dflong2 and dflong3 look different as they are sorted by different variable ## this is just a visual difference but we can fix it anyway dflong3 &lt;- dflong3 %&gt;% arrange(participant, group) As I said we do not need to revert back to the long format. We will carry on working with the wide format and calculate our diff variable. dfwide &lt;- dfwide %&gt;% mutate(diff = Gambling - Neutral) It would be very interesting now to see whether this difference correlates with the frequency of games. Lets also load the questionnaire.csv and merge it with our widedf. ## load our datafile questionnaire &lt;- read_csv(&quot;questionnaire.csv&quot;) ## let&#39;s merge the two datasets dfwide and questionnaire ## first lets do it the wrong way dfall &lt;-left_join(dfwide, questionnaire) ## this will generate an error as group in questionnaire is not a factor ## also group in the questionnaire is 1 and 2 not the same as in dfwide ## also left_join() is not the appropriate function for our case questionnaire &lt;- questionnaire %&gt;% mutate(group = recode(group,&#39;1&#39; = &quot;gambling&quot;, &#39;2&#39;=&quot;control&quot;), group = as.factor(group)) dfwide$group = as.factor(dfwide$group) ## let&#39;s merge the two datasets dfwide and questionnaire dfall &lt;-full_join(dfwide, questionnaire) That worked fine, even though we have some missing values. Lets see now if we have any significant correlation between the stroop difference and the frequency of games. Let s start with a simple scatterplot. scatter &lt;- ggplot(dfall, aes(x = games, y = diff)) + geom_point() scatter This looks rather unexciting, perhaps group can make things more interesting. scatter &lt;- ggplot(dfall, aes(x = games, y = diff, color = group)) + geom_point() scatter Lets carry out the correlational analysis next. library(Hmisc) dfgambling &lt;- dfall %&gt;% filter(group == &quot;gambling&quot;) rcorr(dfgambling$games, dfgambling$diff) "]]
